---
title: "Learning to use Zooniverse data"
author: "Erika Barthelmess"
date: "11/28/2018"
output: html_document
---
###Introduction
We are using the [Zooniverse](https://zooniverse.org) platform to score pictures
from game cameras.  The process for getting the pictures into the platform is pretty straightforward though somewhat time consuming.  However, once you download the classification, the difficulty goes way up.

Data arrive as a csv file in JSON format.  Some columns in the data are very straighforward
and act just like "normal" columns in e.g. an excel spreadsheet or an R data frame.
However, other columns, especially the "annotation" column (which contains all of the interesting classification info such as species ID) comes in as a JSON "array".  Each row in the column contains an array with mutiple pieces of information.  Sort of like a new csv in a single column.

This R project is set up so that I can work through data preparation for some example zooniverse projects to start
understanding how to deal with the files.  Much of the information I'm using and working with comes from the Zooniverse [Data-Digging-Code-Repository](https://github.com/zooniverse/Data-digging) on GitHub.  In particular, the links there to Ali Burchard's R code in her [Data Processing git repository](https://github.com/aliburchard/DataProcessing) is being used or modified here. I've downloaded the files from this git repository and dropped them in the folder for this project.

###Example Project: Michigan

Here I'm working with example data and R code from Ali Burchard that walks through how to flatten and deal with aggregations in the Michigan Wildlife Watch project. Here is the [RPub from Ali Burchard](http://www.rpubs.com/aliburchard/283030) that deals with the same topic.

#### Import and flatten the zooniverse output file
Below I'm copying the code from the Michigan project: michigan-flatten.R

####Step 1
Set up the workspace
```{R}
rm(list = ls())
#devtools::install_github("sailthru/tidyjson") #install tidyjson if needed
library(tidyjson)
library(magrittr)
library(jsonlite)
library(dplyr)
library(stringr)
library(tidyr)
library(lubridate)

source(file = "DataProcessing-master/projects/survey-tasks/generalized/flattening_script.R")
```
####Step 2 Clean classification data and specify fields

First specify the project
```{R}
project_name <- "michigan"
classifications_file <- "DataProcessing-master/projects/sample_data/michigan-sample.csv"
```

Next load and take a quick look at the data
```{R}
jdata <- read.csv(classifications_file, stringsAsFactors = F)
head(jdata)
```
Now examine the data and set some project-specific details
```{R}
# Set project-specific details
check_workflow(jdata) %>% View
workflow_id_num <- 2276 #the workflow ID number comes from looking at the DF
workflow_version_num <- 463.55 #the workflow version number comes from looking at theDF
```
So far so good.  But it is possible to have more than one workflow and more than one workflow in the same data export.  So we need to specify that we just want to work with data for the relevant workflow and version, as just specified above.
```{R}
# limit to relevant workflow id and version
jdata <- jdata %>% filter(., workflow_id == workflow_id_num, workflow_version == workflow_version_num)
```
Now we need to looks at some variables associated with different workflow tasks.  I don't wholly understand this, but if you look at the raw data frame, you can see that within the annotations column there are, in each rows, tasks defined by numbers such as "{"task":"T3","value":[{"choice":"COTTONTAILRABBIT",...."

```{R, echo = F}
# Identify task-specific details. These variable names are important, because I haven't figured out how to define them in the function call 
# (there's some weird referencing. I don't know. The function definitions and scripts could be improved, but things seem to generally work.)
View_json(jdata)
#the next part comes from viewing the json data uwing View_json
survey_id <- c("T3")
single_choice_Qs <-  c("HOWMANYANIMALSDOYOUSEE")
single_choice_colnames  <-  c("how_many")
multi_choice_Qs <- c("WHATISTHEANIMALSDOING")
multi_choice_colnames <- c("behavior")
```
####Step 3. Flatten the file
Now that we have assigned some variables, it is time to flatten the file.
```{R}
# Flatten by calling the code from the flattening_functions file. This isn't the cleanest approach, but it'll have to do.
# If you want to combine multiple workflows or multiple tasks before aggregating, this is the time to do it.
final_data <- run_json_parsing(data = jdata)

View(final_data)
```
####Step 4. Save the result
```{R}
write.csv(final_data, file = paste0(classifications_file, "-flattened.csv"), row.names = F)
```
The Data out in the final_data data frame will be in the following format:

* Metadata
* Species (Choice)
* single choice questions
* multiple choice questions, with the colnames prepended to the values
* subject_ids: unique Zooniverse subject identifier. you will need to link this back to your primary key via subject metadata.
* user_name: registered user name or "not-logged-in-hash" where hash is the user's hashed IP address
* classification_id: a unique key representing that classification. This will be unique to the user and subject, but can encompass multiple tasks
* workflow_version: the major and minor version of the workflow
* task_index: an index of tasks. Usually will be 1 if this is your only task.
* task: the task identifier, e.g. "T0"
* task_label: <NA> for survey tasks, but for follow-up questions, this would be the text of the question itself
* value: the annotation data in an embedded list (that is saved as a character). This is really just for double checking against.
* total_submissions: The total number of submissions a user made for a give species/choice. So, if they said lion, 1, standing and leopard, 1, standing, this = 2.
* submission_index: Reflects the index of the particular choice. Not really important.
* choice: Your species choice. NOTE that if you have multiple workflow versions and change species names, you'll need to reconcile these.
* how_many: note that this is not actually a NUMBER, and be careful that you don't treat it as one, especially if you have ranges like 3-5 that get saved as 35.
* behavior_EATING: Every possible answer for a "select all that apply" question gets it's own column, so that you can calculate the proportion of users who marked them present.
* behavior_INTERACTING
* behavior_MOVING
* behavior_RESTING
* behavior_STANDING

Ok!  This has worked so far.  Now it is time to try with my data!
###Example Project: Kenya
Now I'll monkey around with some kenya data and a flatten file that is more mature from the Zooniverse git hub [here](https://github.com/zooniverse/Data-digging/tree/master/example_scripts/R_code/survey-tasks/kenya-wildlife-watch).

Here is the code from Kenya-flatten.R
####Step 1. Set up the workspace and load data
Begin by setting up the workspace
```{R}
rm(list = ls())

library(tidyjson)
library(magrittr)
library(jsonlite)
library(dplyr)
library(stringr)
library(tidyr)
library(lubridate)

source(file = "DataProcessing-master/projects/survey-tasks/generalized/flattening_script.R")
```
####Step 2 Clean classification data and specify fields
First specify the project
```{R}
# Specify Project
project_name <- "kenya"
classifications_file <- "kenya-sample.csv"
```
Next load and take a quick look at the data
```{R}
# Load and Examine data
jdata <- read.csv(classifications_file, stringsAsFactors = F)
head(jdata)
```
Now set some project-specific details
```{R}
# Set project-specific details
check_workflow(jdata) %>% View
workflow_id_num <- 2030
workflow_version_num <- 307.77
```

So far so good.  But it is possible to have more than one workflow and more than one workflow in the same data export.  So we need to specify that we just want to work with data for the relevant workflow and version, as just specified above.
```{R}
# limit to relevant workflow id and version
jdata <- jdata %>% filter(., workflow_id == workflow_id_num, workflow_version == workflow_version_num)
```
Now we need to look at some variables associated with different workflow tasks.  I don't wholly understand this, but if you look at the raw data frame, you can see that within the annotations column there are, in each rows, tasks defined by numbers such as "{"task":"T3","value":[{"choice":"COTTONTAILRABBIT",...."

```{R}
# Identify task-specific details. These variable names are important, because I haven't figured out how to define them in the function call
# (there's some weird referencing. I don't know. The function definitions and scripts could be improved, but things seem to generally work.)

View_json(jdata)
survey_id <- "T3"
single_choice_Qs <-  c("HOWMANY", "ARETHEREANYYOUNGPRESENT") # if the length of the columname vectors don't match, you'll get an error
single_choice_colnames  <-  c("how_many", "young")
multi_choice_Qs <- c("WHATBEHAVIORSDOYOUSEE")
multi_choice_colnames <- c("behavior")

shortcut_id <- "T5"
```
####Step 3 Flatten the file
Now that we have assigned some variables, it is time to flatten the file.

```{R}
# Flatten by calling the code from the flattening_functions file. This isn't the cleanest approach, but it'll have to do.
# If you want to combine multiple workflows or multiple tasks before aggregating, this is the time to do it.
survey_data <- run_json_parsing(data = jdata)
```
At this point, we have parsed the annotation column so that we know both what the animal is and what behaviors it is exhibiting.  Now let's deal with the other tasks.

Task 5 looks like this in the JSON format:
{
        "task": "T5",
        "task_label": null,
        "value": [
            "Nothing Here"
        ]
    }
    
So to get that question back, we do the following:
```{R}
# ADD the nothing here questions back in.
# Eventually would be good to add this bit back into the flattening script itself, though it could get complicated if multiple answers for a shortcut question, like in Serengeti.
final_data <- flatten_to_task(json_data = jdata) %>%
     filter_to_task(task_id = shortcut_id) %>%
     flatten_shortcut(.) %>% select(classification_id, string) %>%
     left_join(survey_data, .) %>%
     rename(empty = string)

View(final_data)
```
####Step 4 Save the result
```{R}
write.csv(final_data, file = paste0(project_name, "-flattened.csv"))
```

Ok, this all worked for the kenya example.  Let's see if I can apply the principles on my data. 


