---
title: "Test with My Data"
author: "Erika"
date: "11/29/2018"
output: html_document
---
###Introduction
We are using the [Zooniverse](https://zooniverse.org) platform to score pictures
from game cameras.  The process for getting the pictures into the platform is pretty straightforward though somewhat time consuming.  However, once you download the classification, the difficulty goes way up.

Data arrive as a csv file in JSON format.  Some columns in the data are very straighforward
and act just like "normal" columns in e.g. an excel spreadsheet or an R data frame.
However, other columns, especially the "annotation" column (which contains all of the interesting classification info such as species ID) comes in as a JSON "array".  Each row in the column contains an array with mutiple pieces of information.  Sort of like a new csv in a single column.

This R project is set up so that I can work through data preparation for some example zooniverse projects to start
understanding how to deal with the files.  Much of the information I'm using and working with comes from the Zooniverse [Data-Digging-Code-Repository](https://github.com/zooniverse/Data-digging) on GitHub.  In particular, the links there to Ali Burchard's R code in her [Data Processing git repository](https://github.com/aliburchard/DataProcessing) is being used or modified here. I've downloaded the files from this git repository and dropped them in the folder for this project.

In the "ProjectIntroduction.Rmd" file associated with this project, I used the above stuff to run a test with some data from a Michigan project.  Here I'm going to play with modifying that code to try to flatten my own data.

### Trying it out
This approach involves using the code I used for the practice data to get the species information and then code from Ivan Ramler to obtain and associate the habitat and transect data.

#####Step 1
Set up the workspace and load functions
```{R}
rm(list = ls())
#devtools::install_github("sailthru/tidyjson") #install tidyjson if needed
library(tidyjson)
library(magrittr)
library(jsonlite)
library(dplyr)
library(stringr)
library(tidyr)
library(lubridate)
library(purrr) #needed for obtaining habitat data
library(readr)
source(file = "DataProcessing-master/projects/survey-tasks/generalized/flattening_script.R")
```
Nothing in the above needed changing.
#####Step 2 Clean classification data and specify fields

First specify the project
```{R}
project_name <- "mammalogy"  ## change project name
classifications_file <- "test1-workflow-classifications.csv" ##Change input file
```

Next load and take a quick look at the data
```{R}
jdata <- read_csv(classifications_file)
head(jdata)
```
Now examine the data and set some project-specific details
```{R}
# Set project-specific details
check_workflow(jdata) %>% View
workflow_id_num <- 8071 #the workflow ID number comes from looking at the DF
workflow_version_num <- 20.19 #the workflow version number comes from looking at the DF.  Note that you should open the file and look at it with view(jdata) because just looking at the description in the environment window yoy may see rounding.
```
So far so good.  But it is possible to have more than one workflow and more than one workflow in the same data export.  So we need to specify that we just want to work with data for the relevant workflow and version, as just specified above.
```{R}
# limit to relevant workflow id and version
jdata <- jdata %>% filter(., workflow_id == workflow_id_num, workflow_version == workflow_version_num)
```
##### Step 3 Assign varibles from annotation column
Now we need to looks at some variables associated with different workflow tasks. I don't wholly understand this, but if you look at the raw data frame by running View_json(), you can see that within the annotations column there are, in each rows, tasks defined by numbers such as "{"task":"T3","value":[{"choice":"COTTONTAILRABBIT",...."

View the file
```{R}
View_json(jdata)
```
From the View_json call, I can see that my workflow has four tasks: T0, T1, T2 and T3. The next part comes from viewing the json data using View_json.

I'm going to base the next steps on the kenya example from ProjectIntroduction.Rmd since that code deals with multiple tasks. Since the example code just dealt with one task, I need to modify it.  
######First get classification data
```{R}
# Identify task-specific details. These variable names are important, because I haven't figured out how to define them in the function call 
# (there's some weird referencing. I don't know. The function definitions and scripts could be improved, but things seem to generally work.)
survey_id <- c("T1") 
single_choice_Qs <-  c("HOWMANY")
single_choice_colnames  <-  c("how_many")
multi_choice_Qs <- c("WHATBEHAVIORSDOYOUSEE")
multi_choice_colnames <- c("behavior")
```

#####Now get subject data before flattening the file
Here's the code I used from Ivan:
```{r}
subjects<-purrr::map(jdata$subject_data, jsonlite::fromJSON, flatten = T)
#this brought in the subject_data as a list of 75 items
habitat<-sapply(subjects, function(x)x[[1]]$Habitat, simplify = T)
transect<-sapply(subjects, function(x)x[[1]]$`#transect`, simplify = T)
jdata$habitat<-habitat
jdata$transect<-transect

```

Now save that file as .csv to examine in excel
```{r}
write.csv(jdata, file = "jdata_w_transect.csv", col.names = T)
```

######Flatten the file
Now that we have assigned some variables and have the habitat information, it is time to flatten the file.
```{R}
# Flatten by calling the code from the flattening_functions file. This isn't the cleanest approach, but it'll have to do.
# If you want to combine multiple workflows or multiple tasks before aggregating, this is the time to do it.

#now break down run_json_parsing into steps rather than a single function

   # Now run through all of functions to flatten everything
# FLATTEN TO TASK
# Highest order flattening to extract relevant tasks *within* a classification. 

flat_to_task <- jdata %>% 
          select(., subject_ids, user_name, classification_id, workflow_version,
                 habitat, transect, annotations)  %>%
          as.tbl_json(json.column = "annotations") %>%
          gather_array(column.name = "task_index") %>% # really important for joining later
          spread_values(task = jstring("task"), task_label = jstring("task_label"), value = jstring("value"))
     
 
flattened <- flat_to_task %>% filter_to_task(task_id = survey_id) #Produces one row per classification. Useful when wanting to recombine other, potentially breaking, task types.
     choices_only <- get_choices(flattened) # grabs all of the choices. Can produce >1 row per classification.
     single_choice_answers <- get_single_choice_Qs(choices_only, cols_in = single_choice_Qs, cols_out = single_choice_colnames) #cols_out is optional
     multi_choice_answers <- get_multi_choice_Qs(choices_only, cols_in = multi_choice_Qs, cols_out = multi_choice_colnames) #cols_out is optional
     
     # now combine everything
     full_data <- flattened
     if(!is.null(choices_only)) full_data <- left_join(flattened, choices_only)
     if(!is.null(single_choice_answers)) full_data <- left_join(full_data, single_choice_answers)
     if(!is.null(multi_choice_answers))  full_data <- left_join(full_data, multi_choice_answers)
     

View(final_data)
```
This gives me the info I need for Mammalogy
Save file
```{R}
write.csv(full_data, "final_test_camera_data.csv")
```
######Now look at other Tasks
Begin by assigning each additional task a shortcut id
```{r}
shortcut_id1<-"T0"
shortcut_id2<-"T2"
shortcut_id3<-"T3"
```
Next add these "nothing" questions back in (Is there precipitation? Is there an animal in the photo?  Does the time stamp look correct?).  Start by doing it three times - if it works, could make it a loop
```{R}
# ADD the nothing here questions back in.
# Eventually would be good to add this bit back into the flattening script itself, though it could get complicated if multiple answers for a shortcut question, like in Serengeti.
final_data <- flatten_to_task(json_data = jdata) %>%
     filter_to_task(task_id = shortcut_id1) %>%
     flatten_shortcut(.) %>% select(classification_id, string) %>%
     left_join(survey_data, .) %>%
     rename(empty1 = string)
```
Keeps crashing, so break it apart to see which step is giving me a headache
```{r}
Step1<-flatten_to_task(json_data = jdata)
```
That works
```{r}
Step2<-filter_to_task(Step1, task_id = shortcut_id1)
```
Also works
```{r}
Step3<-flatten_shortcut(Step2) %>% select(classification_id, string) %>%
     left_join(survey_data, .) %>%
     rename(empty1 = string)
```
Stuck with step 3, but right now it doesn't matter.  final_data has what I need.

###Now try aggregating the data:

THIS CODE SHOULD WORK on most projects, but will need tweaking. It is not as standalone as the flattening_wrapper

```{R}
rm(list = ls())
library(dplyr)
library(magrittr)
library(stringr)
library(tidyr)
source("DataProcessing-master/projects/survey-tasks/generalized/aggregate_functions.R")
```

```{R}
raw_data <- read.csv("final_test_camera_data.csv")
# note that the total_species count reported here isn't accurate when users report the same species multiple times. 
head(raw_data)

raw_data %>% summarise(n_distinct(subject_ids), n_distinct(classification_id)) 

raw_data %<>% 
     group_by(subject_ids) %>% # count up the number of distinct classification IDs
     mutate(., num_class = n_distinct(classification_id)) %>% #because there will be >1 row per classification_id if >1 spp
     arrange(., subject_ids, classification_id) 
raw_data %>% View
```


Need to Identify behavior columns, how many columns, etc. Let's get data input out of the way now.

```{R}
howmany_column <- "how_many" # this is a special kind of single-answer column. 
multi_answer_cols <- names(select(ungroup(raw_data), starts_with("behavior"))) #the flattening script handily appends this to the front of behavior columns.
yesno_columns <- NULL # no single-answer columns here, other than how many, which gets special treatment.
```

NOTE THAT YOU NEED TO PROVIDE THE MAPPING LATER ON IF YOU USE A HOW MANY COLUMN. I CAN'T SEEM TO GET THE FUNCTION TO ACCEPT VARIABLES. 
howmany_map_from <- c("1", "2", "35", "610", "MANY")
howmany_map_to <- c("1", "2", "4", "8", "20") # need to provide a numeric map (at least until I can write a function to get medians for ordered factors)
lookup_list <- as.character("'1' = '1', '2' = '2', '35' = '4', '610' = '8', 'MANY' = '20'")



########### CLEAN UP MULTIPLE VOTES PER USER ###############
Number of different species should match the number of submissions per user.
research team needs to decide how to combine duplicate entries per user. 
Easiest thing is to just take the first submission, though most robust thing is probably to combine the answers.
Note: this will be difficult without knowing what is a count, etc. Research teams can create their own, or, hoping it's relatively rare, just drop the extras.

```{R}
check_spp_counts <- raw_data %>% 
     group_by(subject_ids, classification_id) %>% 
     mutate(., num_species = n_distinct(choice), check_num_spp = n()) 

#check for duplicates
bad_counts <- check_dups(raw_data)
```

can just run this - sets cleaned_classifications to the correct dataset, dropping duplicates where necessary
```{R}
if(is.null(dim(bad_counts))) {
     print("No duplicates to drop")
     cleaned_classifications <- check_spp_counts
} else {
     # NOTE that I don't know how you combine different answers for a single choice questions, thus, this just takes the FIRST anser
     print(paste("Dropping", dim(check_dups(raw_data))[1], "duplicate classifications"))
     
     cleaned_classifications <- raw_data %>% group_by(subject_ids, classification_id) %>% 
          mutate(., num_species = n_distinct(choice)) %>%
          group_by(., subject_ids, classification_id, num_class, num_species, choice) %>% 
          #summarise_all(., sum) # adds up counts for duplicates of spp, only works if everything is numeric
          summarise_all(., first) # takes the first record per user per species classification
}



check_dups(cleaned_classifications)
```


####################### AGGREGATE! #######################

 
### SUBJECT-LEVEL METRICS
```{R}
subject_metrics <- cleaned_classifications %>% ungroup %>%
     group_by(., subject_ids) %>%
     mutate(., num_votes = n(), # if a  user ids >1 spp, there will be more votes than classifications
            diff_species = n_distinct(choice)) # count the total number of different species reported by different users, for pielous score

glimpse(subject_metrics)

# Calculate aggregate number of species per subject by taking the median number of species reported across all volunteers, and tie back to subject metrics.
species_counts <- cleaned_classifications %>% ungroup %>%
     group_by(subject_ids, classification_id) %>%
     summarise(total_spp_by_user = mean(num_species)) %>% #Need to select only one row per classification_id, then summarise across those. 
     summarise(., agg_num_species = round(median(total_spp_by_user), 0))#aggregate species count, which is median rounded up
     glimpse(species_counts)

cleaned_classifications <- left_join(subject_metrics, species_counts) %>% ungroup
glimpse(cleaned_classifications)
```

### SPECIES-LEVEL METRICS

For each species, aggregate counts and behavior votes. 
Okay, so there's a difference between the proportion of VOTES and the proportion of classifications. 
If some users ID >1 species in a single species image, there will be more votes than classifications. 
The opposite is true for when some users only ID 1 species in a multi-species image.


This code snippet provides one row per species ID per classification. We actually don't really need all the grouping variables... could just pull them apart and save for later.
```{R}
grouped_classifications <- cleaned_classifications %>% 
     select(., -num_species) %>% # these aren't relevant
     group_by(., subject_ids, num_class, num_votes, agg_num_species, diff_species, choice) # fields at subject level or higher

#Tally the votes for each species ID'd within a subject
species_votes <- grouped_classifications %>% 
     # for every species within a subject, aggregate votes.
     summarise(., votes = n_distinct(classification_id)) %>% #count up the number of votes per species choice
     mutate(propvote = votes/sum(votes), #calculate proportion of votes for this species
            propclass = votes/num_class) #calculate proportion of classifications for this species
```
Tally votes for factor questions with single YES OR NO answers. STILL NEED to create a function to calculate proportions for different answer types.

```{R}
question_votes <- grouped_classifications %>% 
summarise_at(., .cols = yesno_columns, funs(calc_yes))
```
Not working

Tally votes for the different behaviors (or other multi-choice feature) for each species.
```{R}
multi_answer_votes <- grouped_classifications %>%
     summarise_at(., .cols = multi_answer_cols, funs(calc_prop))

howmany_votes <- grouped_classifications %>%
     mutate(how_many = dplyr::recode(as.character(how_many), '1' = '1', '2' = '2', '35' = '4', '610' = '8', 'MANY' = '20')) %>%
     mutate(how_many = as.numeric(how_many)) %>%
     summarise_at(., .cols = howmany_column, funs(med_count = median, min_count = min, max_count = max))
```

Okay, so the full dataset has all of the aggregate votes per species. The only thing left is to select the top n species for each subject.

```{R}
all_data <- full_join(species_votes, howmany_votes) %>% full_join(., multi_answer_votes)

write.csv(final_dat, file = "final_test_mammalogy.csv")
```

Save some output    
```{R}
write.csv(check_spp_counts, "final_test_results.csv")
```











